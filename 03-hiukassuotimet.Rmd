\chapter{Hiukkassuotimet} \label{hiukkassuotimet}

\section{SIR-algoritmi}

Tässä alaluvussa esitetään epälineaarisen suodinongelman ratkaisemiseksi SIR-hiukkassuodinalgoritmi. Algoritmi on numeerinen toteutus luvussa \@ref(bayesilainen-suodin) kuvatusta Bayesilaisesta suotimesta. Esitetty algoritmi perustuu Gustafssoniin (2010) [@gustafsson-2010]. Ilman uudelleenotantavaihetta kyseessä olisi SIS-algoritmi.

Algoritmi alustetaan jakaumasta $x_1^i\sim p_{x_0}$ generoiduilla $N$ kappaleella partikkeleita. Jokaiselle partikkelille annetaan alustuksessa sama paino $w_{1|0}^i=1/N$. Algoritmi suoritetaan jokaiselle partikkelille $i=\{1,2,\ldots,N\}$ jokaisella aika-askeleella $k=\{1,2,\ldots,T\}$.

Algoritmin ensimmäisessä vaiheessa päivitetään painot yhtälön (\ref{painopaivitys}) mukaan.

\begin{align}\label{painopaivitys}
w^i_{k|k}=\frac{1}{c_k}w^i_{k|k-1}p(y_k|x^i_k).
\end{align}

\noindent Tämä vastaa edellä esitetyn Bayes-suotimen päivitysvaihetta (\ref{bayes-paivitys}). Normalisointipaino $c_k$ lasketaan puolestaan yhtälöstä (\ref{normalisointi}), mikä vastaa Bayes-suotimen normalisointivakion laskemista (\ref{bayes-normalisointi}) ja asettaa painojen summaksi $\sum_{i=1}^Nw^i_{k|k}=1$.

\begin{align}\label{normalisointi}
c_k=\sum_{i=1}^{N}w_{k|{k-1}}^ip(y_k|x_k^i).
\end{align}

\noindent Seuraavassa vaiheessa estimoidaan $p$ laskemalla tiheyden $p(x_{1:k}|y_{1:k})$ Monte Carlo -estimaatti yhtälön (\ref{p-estimaatti}) perusteella

\begin{align}\label{p-estimaatti}
\hat{p}(x_{1:k}|y_{1:k})=\sum_{i=1}^{N}w_{k|k}^i \delta(x_{1:k}-x_{1:k}^i).
\end{align}

Tämän jälkeen suoritetaan valinnainen uudelleenotanta. Uudelleenotanta voidaan tehdä jokaisella askeleella tai efektiivisen otoskoon perusteella alla kuvatun kynnysarvoehdon $\hat{N}_{eff}< N_{th}$ täyttessä, jolloin uudelleenotantaa kutsutaan adaptiiviseksi uudelleenotannaksi. Tällaista uudelleenotantaa hyödynnetään esitetyssä algoritmissa (\ref{sir}). Lopuksi päivitetään aika-askel (jos $k < T$), luodaan uudet ennusteet partikkeleille ehdotusjakaumasta (\ref{ehdotusjakauma})

\begin{align}\label{ehdotusjakauma}
x_{k+1}^i\sim q(x_{k+1}|x_k^i,y_{k+1})
\end{align}

\noindent ja päivitetään partikkelien painot tärkeytysotannalla (\ref{tarkeytys}) sen mukaan kuinka todennäköisiä partikkelien ennusteet ovat 

\begin{align}\label{tarkeytys} w_{k+1|k}^i=w_{k|k}^i\frac{p(x_{k+1}^i|x_k^i)}{q(x_{k+1}^i|x_k^i,y_{k+1})}.
\end{align}

\noindent Vaiheet (\ref{ehdotusjakauma}) ja (\ref{tarkeytys}) vastaavat Bayes-suotimen aikapäivitystä (\ref{bayes-aikapaivitys}). 

Alla käsitellään algoritmiin liittyvän uudelleenotantamenetelmän, partikkelien määrän ja ehdotusjakauman valinta. Lopuksi esiteetään algoritmin konvergenssia, marginaalijakaumaa sekä aikakompleksisuutta koskevia tuloksia ja käsitellään algoritmin tuottaman jakaumaestimaatin varianssin estimointia.

\begin{algorithm}[H]
\label{sir}
\DontPrintSemicolon
\SetAlgoShortEnd
\KwResult{Posteriorijakauman $p(x_{1:k}|y_{1:k})$ estimaatti.\;}
\KwData{Havainnot $y_k$. Generoitu $x_1^i\sim p_{x_0}$ missä $i=\{1,\ldots,N\}$ ja jokainen partikkeli saa saman painon $w_{1|0}^i=1/N$.\;}
\Begin{
  \For{$k=\{1,2,\ldots,T\}$}{
    \For{$i=\{1,2,\ldots,N\}$}{
      \Begin{Päivitetään painot $w_{k|k}.$\;}
      \Begin{Estimoidaan $p$ laskemalla tiheydelle approksimaatio $\hat{p}(x_{1:k}|y_{1:k})=\sum_{i=1}^{N}w_{k|k}^i \delta(x_{1:k}-x_{1:k}^i)$.\;}
    }
    \Begin{Lasketaan efektiivinen otoskoko $\hat{N}_{eff}$.\;}
    \If{$\hat{N}_{eff}< N_{th}$}{\Begin{Otetaan uudet $N$ otosta palauttaen joukosta $\{x_{1:k}^i\}_{i=1}^N$, missä otoksen $i$ todennäköisyys on $w^i_{k|k}$.\;}
    \Begin{Asetetaan painot $w^i_{k|k}=1/N$.\;}}
    \If{$k < T$}{\Begin{Aikapäivitys. \newline Luodaan ennusteet partikkeleille ehdotusjakaumasta $x_{k+1}^i\sim q(x_{k+1}|x_k^i,y_{k+1})$, \newline päivitetään partikkelien painot tärkeytysotannalla.\;}}
  }  
}
\caption{SIR}
\end{algorithm}

\subsection{Suunnitteluparametrien valinta}

Ennen algoritmin suorittamista valitaan ehdotusjakauma $q(x_{k+1}|x_{1:k},y_{k+1})$, uudelleenotantamenetelmä sekä partikkelien määrä $N$. Ehdotusjakauman ja uudelleenotantamenetelmän valinnassa tärkeimpänä päämääränä on välttää otosten ehtymistä, kun taas partikkelien määrä säätelee kompromissia algoritmin suorituskyvyn ja sen tarkkuuden sekä varianssin välillä.

\subsubsection{Otoskoon $N$ valinta}

Yleispätevää sääntöä otoskoon/partikkelien lukumäärän $N$ valinnalle on vaikeaa antaa, sillä vaadittava estimointitarkkuus riippuu usein käsillä olevasta ongelmasta. Gordon \&al. (1993) [@Gordon-1993] esittävät kuitenkin kolme tekijää, jotka vaikuttavat partikkelien lukumäärän valintaan

a. tila-avaruuden ulottuvuuksien lukumäärä ${n_x}$,
b. tyypillinen päällekäisyys priorin ja uskottavuuden välillä
c. sekä tarvittava aika-askelten lukumäärä.

Ensimmäisen tekijän vaikutus on selvä. Mitä useammassa ulottuvuudessa otantaa tarvitsee tehdä, sen korkeammaksi on $N$ asetettava, jotta jokainen ulottuvuus pystytään kattamaan tehokkaasti. Tekijät (\textit{b}) ja (\textit{c}) puolestaan seuraavat uudelleenotannasta. Jos se osa tila-avaruutta, jossa uskottavuus $p(y_k|x_k)$ saa merkittäviä arvoja on pieni verrattuna siihen osaan, jossa priorijakauma $p(x_k|y_{1:k-1})$ saa merkittäviä arvoja, suuri osa partikkeleista saa pieniä painoja eikä näin valikoidu uudelleenotantaan. 

Yleisesti ottaen $N$ kannattaa asettaa sellaiseksi, että se paitsi tuottaa riittävän tarkan estimaatin, on se myös käytettävissä olevan laskentatehon sekä vaadittavan laskentanopeuden kannalta järkevä. Tähän palataan tutkielman empiirisessä paikannusesimerkissä.

\subsubsection{Uudelleenotantamenetelmän valinta} \label{uudelleenotantamenetelman-valinta}

Ilman uudelleenotantaa on mahdollista, että algoritmi alkaa kärsiä SIS-algoritmille tyypillisestä otosten ehtymisestä. Toisin sanoen kaikki painot alkavat keskittyä vain muutamalle partikkelille eikä algoritmi enää approksimoi tehokkaasti haluttua jakaumaa. Uudelleenotanta tarjoaa osittaisen ratkaisun tähän ongelmaan, mutta hävittää samalla informaatiota ja lisää siten satunnaisotantaan liittyvää epävarmuutta. Yleisesti ottaen uudelleenotanta kannattaa aloittaa vasta siinä vaiheessa algoritmin suorittamista, kun siitä on otosten ehtymisen kannalta hyötyä, esimerkiksi efektiivisen otoskoon pudottua jonkin kynnysarvon alapuolelle (adaptiivinen uudelleenotanta). Efektiivinen otoskoko saadaan laskettua variaatiokertoimesta $c_\nu$ kaavalla

\begin{align}\label{N-eff}
N_{eff}= \frac{N}{1+c_\nu^2(w^i_{k|k})} = \frac{N}{1+\frac{\text{Var}(w^i_{k|k})}{(\mathbb{E}[w^i_{k|k}])^2}} =\frac{N}{1+N^2\text{Var}(w^i_{k|k})}.
\end{align}

Näin laskettu efektiivinen otoskoko kuvaa sitä, kuinka tasaisesti painot jakautuvat partikkeleille. Efektiivinen otoskoko maksimoituu ($N_{eff}=N$), kun kaikille painoille pätee $w^i_{k|k}=1/N$ ja minimoituu ($N_{eff}=1$), kun $w^i_{k|k}=1$ todennäköisyydellä $1/N$ ja $w^i_{k|k}=0$ todennäköisyydellä $(N-1)/N$. Normalisoitujen painojen avulla saadaan efektiiviselle otoskoolle aika-askeleella $k$ laskennallinen approksimaatio

\begin{align}\label{N-hat-eff}
\hat{N}_{eff}=\frac{1}{\sum_{i=1}^N(w^i_{k|k})^2}.
\end{align}

Määritelmille ($\ref{N-eff}$) ja ($\ref{N-hat-eff}$) pätee $1 \leq \hat{N}_{eff} \leq N$. Yläraja saavutetaan, kun jokaisen partikkelin paino on sama. Alarajalle päädytään, kun kaikki paino keskittyy yksittäiselle partikkelille. Tästä saadaan määriteltyä algoritmille SIR-uudelleenotantaehto $\hat{N}_{eff}< N_{th}$. Gustafsson (2010) [@gustafsson-2010] esittää uudelleenotannan kynnysarvoksi esimerkiksi $\hat{N}_{th}=2N/3$. 

Uudelleenotanta ei muuta approksimoitavan jakauman $p$ odotusarvoa, mutta se lisää jakauman Monte Carlo -varianssia. On kuitenkin olemassa esimerkiksi osittamiseen perustuvia uudelleenotantamenetelmiä, jotka pyrkivät minimoimaan varianssin lisäyksen. Varianssin pienennysmenetelmät jätetään tämän tutkielman ulkopuolelle.

\subsubsection{Ehdotusjakauman valinta}

Yksinkertaisin muoto ehdotusjakaumalle on $q(x_{1:k}|y_{1:k})$ eli jokaisella algoritmin suorituskerralla käydään läpi koko aikapolku $1$:$k$. Tämä ei kuitenkaan ole tarkoituksenmukaista, erityisesti jos kyseessä on reaaliaikainen sovellutus. Kirjoitetaan sen sijaan ehdotusjakauma muodossa

\begin{align}\label{proposal-factorization}
q(x_{1:k}|y_{1:k})=q(x_k|x_{1:k-1},y_{1:k})q(x_{1:k-1}|y_{1:k}).
\end{align}

Jos yhtälöstä (\ref{proposal-factorization}) poimitaan ehdotusjakaumaksi ainoastaan termi $q(x_k|x_{1:k-1},y_{1:k})$, voidaan tämä kirjoittaa edelleen Markov-ominaisuuden nojalla muotoon $q(x_k|x_{k-1},y_{k})$. Tämä on suodinongelman kannalta riittävää, koska olemme kiinnostuneita posteriorijakaumasta ja arvosta $x$ ainoastaan aika-askeleella $k$ (siloitinongelmassa tarvitsisimme koko polun $x_{1:k}$). Alla tarkastellaan edelleen Gustafssonia (2010) [@gustafsson-2010] seuraten kahta ehdotusjakauman valintatapaa, prioriotantaa (*prior sampling*) sekä uskottavuusotantaa (*likelihood sampling*).

Ennen ehdotusjakauman tarkastelua määritellään mallille signaali-kohinasuhde uskottavuuden maksimin ja priorin maksimin välisenä suhteena

\begin{align}\label{SNR}
\text{SNR}\propto \frac{\text{max}_{x_k}p(y_k|x_k)}{\text{max}_{x_k}p(x_k|x_{k-1})}. 
\end{align}

\noindent Yhdistetään lisäksi ehdotusjakaumia varten yhtälöt (\ref{painopaivitys}) ja (\ref{normalisointi}), jolloin saadaan painojen päivitys muotoon

\begin{align}\label{painopaivitys-propto}
w^i_{k|k} \propto w^i_{k-1|k-1}\frac{p(y_k|x^i_k)p(x_k|x^{k-1})}{q(x_k|x^i_{k-1},y_k)}.
\end{align}

Kun suhde (\ref{SNR}) on matala, on prioriotanta luonnollinen valinta. Tässä käytetään ehdotusjakaumana tilavektorin ehdollista prioria eli

\begin{align}\label{prioriotanta-q}
q(x_k|x_{1:k-1},y_{k})=p(x_k|x^i_{k-1}).
\end{align}

\noindent Yhtälön (\ref{prioriotanta-q}) perusteella saadaan edelleen prioriotannan painoiksi

\begin{align}\label{prioriotanta-w}
w^i_{k|k} = w^i_{k|k-1}p(y_k|x^i_k) = w^i_{k-1|k-1}p(y_k|x^i_k).
\end{align}

Kun signaali-kohinasuhde on kohtalainen tai korkea, on parempi käyttää ehdotusjakaumana skaalattua uskottavuusfunktiota (\ref{uskottavuusotanta-q}). Tarkastellaan ensin tekijöihin jakoa

\begin{align}\label{uskottavuusotanta-factorization}
p(x_k|x^i_{k-1},y_k)=p(y_k|x_k)\frac{p(x_k|x^i_{k-1})}{p(y_k|x^i_{k-1})}.
\end{align}

\noindent Kun SNR on korkea ja uskottavuusfunktio on integroituva pätee $p(x_k|x^i_{k-1},y_{k}) \propto p(y_k|x_k)$, jolloin voidaan asettaa

\begin{align}\label{uskottavuusotanta-q}
q(x_k|x^i_{k-1},y_{k}) \propto p(y_k|x_k).
\end{align}

\noindent Yhtälön (\ref{uskottavuusotanta-q}) perusteella saadaan edelleen uskottavuusotannan painoiksi 

\begin{align}\label{uskottavuusotanta-w}
w^i_{k|k} = w^i_{k-1|k-1}p(x^i_k|x^i_{k-1}).
\end{align}

\subsection{Konvergenssituloksia}

Alla esitetään kolme SIR-algoritmiin liittyvää konvergenssitulosta. Se, kuinka hyvin esitetyllä algoritmilla arvioitu posterioritiheys $\hat{p}(x_{1:k}|y_{1:k})$ approksimoi todellista tiheysfunktiota $p(x_{1:k}|y_{1:k})$, se mikä on approksimaation keskineliövirhe sekä keskeinen raja-arvolause. Tulokset 1\textendash 2 noudattavat Crisanin ja Doucet'n artikkeleita "Convergence of Sequential Monte Carlo Methods" (2000) [@crisan-2000] ja "A Survey of Convergence Results on Particle Filtering Methods for Practitioners" (2002) [@crisan-2002], tulos 3 Chopinin artikkelia "Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference" (2004) [@chopin-2004].

\textit{Konvergenssitulos 1}: Kun $N \to \infty$ algoritmille pätee kaikille aika-askeleille $k$ tulos (\ref{jakaumakonvergenssi}).

\begin{align}\label{jakaumakonvergenssi}
\hat{p}(x_{1:k}|y_{1:k}) \xrightarrow{a.s.} p(x_{1:k}|y_{1:k}).
\end{align}

\textit{Konvergenssitulos 2}: Keskineliövirheelle pätee asymptoottinen konvergenssi (\ref{MSE-konvergenssi}).

\begin{align}\label{MSE-konvergenssi}
\mathbb{E}\left[\hat{g}(x_k)-\mathbb{E}\left[g(x_k)\right]\right]^2\leq\frac{p_k\norm{g(x_k)}}{N},
\end{align}

\noindent missä $g$ on mikä hyvänsä piilossa olevan tila-avaruuden rajoitettu Borel-mitallinen funktio ($g \in \mathcal{B}(\mathbb{R}^{n_x})$), $\norm{g(\cdot)}$ kyseisen funktion supremum-normi ja $p_k$ jokin äärellinen vakio, jolle pätee ajanhetkestä $k$ riippumatta $p_k=p<\infty$. 

\textit{Konvergenssitulos 3}: Keskeinen raja-arvolause (\ref{CLT}).

\begin{align}\label{CLT}
\text{Kun } N \to \infty: \sqrt{N} \left\{ \frac{1}{N} \sum_{i=1}^N \hat{g}(x_k^i) -\mathbb{E}\left[g(x_k^i)\right] \right\} \xrightarrow{D} \mathcal{N}(0,\sigma^2 < \infty),
\end{align}

\noindent missä $g$ on jälleen mikä hyvänsä piilossa olevan tila-avaruuden rajoitettu Borel-mitallinen funktio ($g \in \mathcal{B}(\mathbb{R}^{n_x})$). Konvergenssituloksia ei tämän tutkielman puitteissa todisteta.

\subsection{Marginaalijakauma}

Edellä kuvattu algoritmi \@ref(sir) tuottaa approksimaation koko prosessin posteriorijakaumalle $p(x_{1:k}|y_{1:k})$. Jos halutaan tietää ainoastaan posteriorijakauman $p(x_k|y_{1:k})$ estimaatti, voidaan käyttää yksinkertaisesti viimeisestä tilasta $x_k$ laskettua estimaattia 

\begin{align}
\hat{p}(x_{k}|y_{1:k})=\sum_{i=1}^{N}w_{k|k}^i \delta(x_{k}-x_{k}^i).
\end{align}

Toinen, tarkempi vaihtoehto on käyttää laskennassa tärkeytyspainoa

\begin{align}\label{marginaalitarkeytys}
w_{k+1|k}^i=\frac{\sum_{j=1}^{N}w_{k|k}^jp(x_{k+1}^i|x_k^j)}{q(x_{k+1}^i|x_k^i,y_{k+1})}
\end{align}

\noindent painon (\ref{tarkeytys}) sijaan. Tällöin jokaisella aikapäivitysaskeleella lasketaan painot kaikkien mahdollisten tila-aika-avaruuspolkujen yli. Samoin kuin uudelleenotanta tämä pienentää painojen varianssia, mutta lisää algoritmin aikakompleksisuutta.

\subsection{Aikakompleksisuus}

Algoritmin perusmuodon aikakompleksisuus on $\mathcal{O}(N)$. Uudelleenotantamenetelmän tai ehdotusjakauman valinta ei suoraan vaikuta aikakompleksisuuteen. Sen sijaan marginalisointi tärkeytyspainolla (\ref{marginaalitarkeytys}) lisää algoritmin aikakompleksisuutta $\mathcal{O}(N)\rightarrow\mathcal{O}(N^2)$, koska jokaisen partikkelin kohdalla painot lasketaan jokaisen tila-aika-avaruuspolun yli. On selvää, että erityisesti isoilla otoskoon $N$ arvoilla ei yllä esitetty marginalisointi enää ole mielekästä, erityisesti reaaliaikaisissa sovellutuksissa. 

Tällaisia tilanteita varten algoritmista on olemassa $\mathcal{O}(N\text{log}(N))$-versioita, jotka perustuvat esimerkiksi N:n kappaleen oppimiseen (*N-body learning*). Näiden algoritmien käsittely jää tämän tutkielman ulkopuolelle, mutta katsauksen algoritmeista ovat esittäneet esimerkiksi Klaas \&al. artikkelissa "Toward Practical $N^2$ Monte Carlo: the Marginal Particle Filter" (2005) [@klaas-2005].

\section{Saapasremmisuodin}

Saapasremmisuodin \ref{saapasremmisuodin} eli *bootstrap filter* on SIR-algoritmin muunnelma, jossa tärkeytysotannanssa (kts. \ref{tarkeytysotanta-algo}) käytetään dynaamista mallia $p(x_k|x_{k-1})$. 

\begin{algorithm}[H]
\label{saapasremmisuodin}
\DontPrintSemicolon
\SetAlgoShortEnd
\KwResult{Posteriorijakauman $p(x_{1:k}|y_{1:k})$ estimaatti.\;}
\KwData{Havainnot $y_k$. Generoitu $x_1^i\sim p_{x_0}$ missä $i=\{1,\ldots,N\}$ ja jokainen partikkeli saa saman painon $w_{1|0}^i=1/N$.\;}
\Begin{
  \For{$k=\{1,2,\ldots,T\}$}{
    \For{$i=\{1,2,\ldots,N\}$}{
      \Begin{Luodaan uudet estimaatit dynaamisesta mallista $x_{k}^i\sim p(x_{k}|x_{k-1}^i)$.;}
      \Begin{Päivitetään hiukkasten painot $w_k^i$ uskottavuusfunktion $p(y_k|x_k^i)$ mukaan.}
      \Begin{Estimoidaan $p$ laskemalla tiheydelle approksimaatio $\hat{p}(x_{1:k}|y_{1:k})=\sum_{i=1}^{N}w_{k|k}^i \delta(x_{1:k}-x_{1:k}^i)$.}
    }
    \If{$k < t$}{\Begin{Aikapäivitys. Suoritetaan uudelleenotanta kuten SIR-algoritmissa \ref{sir}.\;}}
  }  
}
\caption{Saapasremmisuodin}
\end{algorithm}

Saapasremmisuodin on edellä esitettyä SIR-algoritmia yksinkertaisempi toteuttaa, mutta epäinformatiivisen tärkeytysjakauman vuoksi algoritmi saattaa vaatia SIR-algoritmia suuremman määrän hiukkasia. Saapasremmisuodin esitetään tässä sen historiallisen tärkeyden vuoksi, sillä kyseessä oli ensimmäinen uudelleenotataantaa hyödyntävä hiukkassuodinalgoritmi. Suotimien käytännön toteutukseen palataan luvussa \@ref(paikannusesimerkki). 

\section{Varianssin estimoinnista} \label{varianssin-estimointi}

Hiukkassuotimen varianssin estimoinnissa ollaan kiinnostuneita jakaumaestimaatin $\hat{p}(x_{1:k}|y_{1:k})$ varianssista. Yksinkertaisin tapa estimoida hiukkassuodinalgoritmin varianssia on ajaa algoritmi $M > 1$ kertaa. Koska ajot ovat toisistaan riippumattomia, voidaan estimaatin varianssi laskea kullekin aika-askeleelle $k$ näiden ajojen $k$-hetken estimaattien otosvarianssina: 

\begin{align}\label{MC-varianssi}
\hat{\sigma}^2_{MC} = \text{Var}(\hat{p}(x_{1:k}|y_{1:k})) = \frac{1}{M-1} \sum_{i=1}^{M}(x_k^i-\bar{x}_k)^2
,\end{align}

\noindent missä $x_k^i$ on $k$:nen aika-askeleen piste-estimaatti ajolle  $i=1,\ldots,M$ ja $\bar{x}_k$ piste-estimaattien aritmeettinen keskiarvo laskettuna kaikkien $M$ ajon yli. Tällaisen Monte Carlo -varianssin estimoiminen on kuitenkin laskennallisesti tehotonta. Monissa käytännön sovellutuksissa jo yhden hiukkassuodinalgoritmin ajaminen vaatii runsaasti laskentatehoa, jolloin Monte Carlo -varianssin laskeminen ei ole mahdollista.

Varianssia ei voida myöskään laskea analyyttisesti, mutta koska keskeisen raja-arvolauseen (\ref{CLT}) nojalla tiedetään, että asymptoottinen varianssi 

\begin{align}\label{asymptoottinen-varianssi}
\lim_{N\to \infty} N \text {Var}(\hat{g}(x_k^i))
\end{align}

\noindent on olemassa, on sen estimointiin kehitetty lähivuosina joitakin approksimatiivisia menetelmiä. Alla käsitellään Chan ja Lain (2013) [@Chan-2013] sekä Leen ja Whitleyn (2018) ehdottamaa varianssin estimointitapaa [@Lee-2018].

Ajetaan SIR-algoritmi kuten esitetty algoritmissa \ref{sir}, mutta merkitään kullakin aika-askeleella $k=1,\ldots,T$ jokaisen partikkelin $n=1,\ldots,N$ kohdalla indeksillä $E_k^n$ kunkin partikkelin kantaisää eli sitä partikkelia, josta kyseinen partikkeli on uudelleenotantojen kautta polveutunut aika-askeleesta $k=1$ lähtien. Kaavio \ref{fig:eeva-indeksit} havainnollistaa tätä partikkelien polveutumista. 

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{eevaindeksit}
\caption{Esimerkki Eeva-indekseistä, kun $T=3$ ja $N=3$.}
\label{fig:eeva-indeksit}
\end{figure}

Varianssiestimaatti voidaan laskea näiden, Leen ja Whitleyn Eeva-indekseiksi nimeämien indeksien perusteella seuraavasti. Merkitään ensin Eeva-indekseillä painotettua summaa

\begin{align}\label{CLE-sum}
\hat{\mu}_k=\sum_{i=1}^N w_k^i x_k^i[E_k]
,\end{align}

\noindent missä merkintä $x_k^i[E_k]$ tarkoittaa, että partikkeleista $x_k$ valitaan Eeva-indeksien osoittamat partikkelit eli ne partikkelit joille $i=E_k^j$, kun $j=1,\ldots,N$. Sama partikkeli voidaan luonnollisesti valita summaan useamman kerran. Tämän perusteella lasketaan edelleen varianssi

\begin{align}\label{CLE-varianssi}
\hat{\sigma}^2_{CLE} = \frac{1}{N(N-1)} \sum_{i=1}^N w_k^i (x_k^i[E_k]-\hat{\mu}_k)^2
.\end{align}

\noindent Kyseessä harhaton ja konsistentti asymptoottisen varianssin estimaatti. Tarkemmin $N\hat{\sigma}^2_{CLE}$ konvergoi asymptoottiseen varianssiin (\ref{asymptoottinen-varianssi}). Tätä ei tutkielman puitteissa todisteta. 

Yllä esitetty varianssiestimaatti saattaa kuitenkin kärsiä indeksien ehtymisestä. Toisin sanoen kun $k \to \infty$, polveutuvat kaikki hiukkaset lopulta samasta kantaisästä eli indeksit $E_k^i,\ldots,E_k^N$ ovat kaikki yhtäsuuria. Tämän vuoksi on mielekästä johtaa indeksit ainoastaan tietystä aiemmasta aika-askeleesta aika-askeleen $k=1$ sijaan. Olsson ja Douc (2019) [@olsson-2019] ehdottavat tähän tarkoitukseen Henok-indeksiä $E_{m,k}^n$, jossa $m$ merkitsee aika-askeleen $m\leq k$ partikkelia, josta kyseinen partikkeli polveutuu. Partikkelien kantaisien sukupolvi määritetään viipeellä $\lambda$ niin, että $m=k-\lambda$. Kaavio \ref{fig:henok-indeksit} havainnollistaa partikkelien polveutumista, kun $\lambda=1$. 

\begin{figure}[H]
\centering
\includegraphics[width=9cm]{henokindeksit_korjattu}
\caption{Esimerkki Henok-indekseistä, kun $T=3$, $N=3$ ja $\lambda=1$.}
\label{fig:henok-indeksit}
\end{figure}

Nyt varianssi voidaan laskea kuten yhtälössä (\ref{CLE-varianssi}) edellä, mutta Eeva-indeksit korvataan Henok-indekseillä. Saadaan painotettu summa

\begin{align}\label{OD-sum}
\hat{\mu}_k=\sum_{i=1}^N w_k^i x_k^i[E_{k(\lambda)}]
\end{align}

\noindent ja varianssi

\begin{align}\label{OD-varianssi}
\hat{\sigma}^2_{OD} = \frac{1}{N-1} \sum_{i=1}^N w_k^i (x_k^i[E_{k(\lambda)}]-\hat{\mu}_k)^2
,\end{align}

\noindent missä $k(\lambda) \vcentcolon= k-\lambda$ eli se aika-askel, jota käytetään hiukkasten kantaisänä. Viive $\lambda$ on varianssiestimaatin suunnitteluparametri. Pienillä viipeillä estimaatti on harhainen, mutta harha laskee viipeen kasvaessa. Olsson ja Douc suosittavat viipeen ylärajaksi arvoa $\lambda=20$, jolloin estimaattorin harha on käytännössä kokonaan hävitetty. Tätä suuremmilla viivearvoilla estimaatti voi alkaa kärsiä samasta epätarkkuudesta kuin Eeva-indekseihin perustuva CLE-estimaatti.

Mastrototaro ja Olsson (2023) [@Mastrototaro-2023] laajentavat estimaattia edelleen niin, että viive $\lambda$ valitaan mukautuvasti. Mastrototaron ja Olssonin ALVar-estimaatti (*Adaptive-Lag Variance*) lasketaan kuten OD-varianssi (\ref{OD-varianssi}) edellä, mutta kunkin algoritmin ajokerran jälkeen asetetaan seuraavan ajanhetken $\lambda$ seuraavasti:

\begin{align}\label{ALVar-lambda}
\lambda_{k+1} \leftarrow \operatorname*{arg\,max}_{\lambda \in [0, \lambda_k + 1]} \hat{\sigma}^2_{k+1,\lambda}
\end{align}

\noindent ja ehtyneet Henok-indeksit määritellään niin, että indeksi on ehtynyt (so. kaikki indeksivektorin indeksit ovat yhtäsuuria), jos jompikumpi seuraavista ehdoista täyttyy:

\
i) Henok-indeksit $E^i_{m,k-1}$ ovat ehtyneet, \
ii) Henok-indeksit $E^i_{m-1,k}$ ovat ehtyneet ja on olemassa $\lambda^\prime \in [0, \lambda-1]$, joka täyttää ehdon $\hat{\sigma}^2_{k+1,\lambda} < \hat{\sigma}^2_{k+1,\lambda^\prime}$, koska myös ehtyneiden indeksien kantaisät ovat ehtyneitä.
\

Nyt indeksi $\lambda_{k+1}$ voidaan valita rekursiivisesti niin, että se tuottaa suurimman varianssiestimaatin, joka on kuitenkin rajoitettu ylhäältä arvoon $\lambda_k+1$. Tämä indeksi ei ole koskaan ehtynyt, joten se on myös parhaan varianssiestimaatin tuottava valinta.

Esitettyjä varianssiestimaatteja voidaan hyödyntää paitsi algoritmien ja parametrivalintojen vertailussa myös mukautuvan viipeen hiukkassiloittimen viipeen valinnassa (kts. luku \@ref(hiukkassiloittimet)). Varianssiestimaatti lasketaan myös luvun \@ref(paikannusesimerkki) empiirisessä esimerkissä.